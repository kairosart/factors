{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Files\n",
    "from util import fetchOnlineData, create_dataset\n",
    "from indicators import get_indicators\n",
    "\n",
    "# TA Library (https://github.com/bukosabino/ta)\n",
    "from ta import *\n",
    "\n",
    "sns.set(font_scale = 1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol = \"AABA\"\n",
    "\n",
    "# We'll look back 365 days\n",
    "start_d = dt.date.today() - dt.timedelta(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute indicators from TA library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yesterday = dt.date.today() - dt.timedelta(1)\n",
    "portf_value = fetchOnlineData(start_d, symbol, yesterday, del_cols=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_all_ta_features(portf_value, \"Open\", \"High\", \"Low\", \"Close\", \"Volume\", fillna=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing index and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete unuseful columns\n",
    "del df['Open']\n",
    "del df['High']\n",
    "del df['Low']\n",
    "del df['Close']\n",
    "del df['Volume']\n",
    "\n",
    "# Create 'date' column for posterior index\n",
    "df['date'] = df.index\n",
    "\n",
    "# Rename column for correlation matrix. Can't have spaces.\n",
    "df.rename(columns={'Adj Close': 'Adj_Close'}, inplace=True)\n",
    "\n",
    "# Reset index\n",
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adj Close correlation matrix\n",
    "corrmat = df.corr()\n",
    "k = 10 #number of variables for heatmap\n",
    "cols = corrmat.nlargest(k, 'Adj_Close')['Adj_Close'].index\n",
    "cm = np.corrcoef(df[cols].values.T)\n",
    "f, ax = plt.subplots(figsize =(12, 10)) \n",
    "sns.heatmap(cm, ax = ax, cmap =\"YlGnBu\", \n",
    "            linewidths = 0.1, yticklabels = cols.values,  \n",
    "                              xticklabels = cols.values) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indicators to use\n",
    "\n",
    "* others_cr: Cumulative Return.\n",
    "* trend_ema_fast: Fast Exponential Moving Averages (EMA) \n",
    "* volatility_kcl: Keltner Channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataframe with indicators to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df[['Adj_Close', 'others_cr', 'trend_ema_fast', 'volatility_kcl']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale data for using reg:logistic\n",
    "If we want use objective = reg: if we want use objective = reg:logistic logistic regression, the label needed to be pre-scaled into [0,1]. logistic regression, the label needed to be pre-scaled into [0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "np_scaled = scaler.fit_transform(dataset)\n",
    "dataset_normalized = pd.DataFrame(np_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Target variable or the desired output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = dataset_normalized.iloc[:,:-1], dataset_normalized.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create XGBoost DMatrix structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dmatrix = xgb.DMatrix(data=X,label=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Parameter optimization\")\n",
    "\n",
    "xgb_model = xgb.XGBRegressor()\n",
    "clf = GridSearchCV(xgb_model,\n",
    "                   {'max_depth': [2,4,6],\n",
    "                    'n_estimators': [50,100,200],\n",
    "                    'learning_rate': [0.01, 0.1, 0.5],\n",
    "                    'min_samples_split':range(200,1001,2000)}, verbose=1)\n",
    "clf.fit(X,y)\n",
    "print('Best Score: ', clf.best_score_)\n",
    "print('Best params: ', clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a hyper-parameter dictionary\n",
    "\n",
    "We will create a hyper-parameter dictionary params which holds all the hyper-parameters and their values as key-value pairs but will exclude the n_estimators from the hyper-parameter dictionary because you will use num_boost_rounds instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'silent':1,\n",
    "          \"objective\":\"reg:logistic\",\n",
    "          'colsample_bytree': 0.3,\n",
    "          'learning_rate': 0.1,\n",
    "          'max_depth': 4,\n",
    "          'alpha': 10,\n",
    "          'min_samples_split': 200,\n",
    "          'n_estimators': 100}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use these parameters to build a x-fold cross validation model by invoking XGBoost's cv() method and store the results in a cv_results DataFrame. Note that here you are using the Dmatrix object you created before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = xgb.cv(verbose_eval=False,\n",
    "                    dtrain=data_dmatrix, \n",
    "                    params=params, \n",
    "                    nfold=10,\n",
    "                    num_boost_round=50,\n",
    "                    early_stopping_rounds=10,\n",
    "                    metrics=\"rmse\", \n",
    "                    as_pandas=True, \n",
    "                    seed=123,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cv_results contains train and test RMSE metrics for each boosting round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(cv_results)\n",
    "plt.xlabel('Boosting round')\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.legend([\"Train mean\", \"Train std.\", \"Test mean\", \"Test std.\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract and print the final boosting round metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((cv_results[\"test-rmse-mean\"]).tail(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that your RMSE for the price prediction has reduced as compared to last time and came out to be lower. \n",
    "\n",
    "You can reach an even lower RMSE for a different set of hyper-parameters. You may consider applying techniques like Grid Search, Random Search and Bayesian Optimization to reach the optimal set of hyper-parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate an XGBoost regressor object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_reg = xgb.train(params=params, dtrain=data_dmatrix, num_boost_round=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the first tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_tree(xg_reg,num_trees=0)\n",
    "plt.rcParams['figure.figsize'] = [18, 6]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize your XGBoost models\n",
    "\n",
    "It's to examine the importance of each feature column in the original dataset within the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_importance(xg_reg)\n",
    "plt.rcParams['figure.figsize'] = [18, 6]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate an XGBoost regressor object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X and y\n",
    "feature_cols = ['others_cr', 'trend_ema_fast', 'volatility_kcl']\n",
    "X = dataset[feature_cols]\n",
    "y = dataset['Adj_Close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split X and y into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale data for using reg:logistic\n",
    "If we want use objective = reg: if we want use objective = reg:logistic logistic regression, the label needed to be pre-scaled into [0,1]. logistic regression, the label needed to be pre-scaled into [0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "y_train_scaled = scaler.fit_transform(np.array(y_train).reshape(-1, 1))\n",
    "y_test_scaled = scaler.fit_transform(np.array(y_test).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_reg = xgb.XGBRegressor(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the regressor to the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_reg.fit(X_train_scaled,y_train_scaled, eval_set=[(X_train_scaled, y_train_scaled), (X_test_scaled, y_test_scaled)],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = xg_reg.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(mean_squared_error(y_test_scaled, preds))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standarize data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = scaler.inverse_transform([preds])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert array to series\n",
    "mylist = preds.tolist()\n",
    "se = pd.Series(mylist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe for plotting\n",
    "predictions = pd.DataFrame()\n",
    "predictions['Price'] = y_test\n",
    "predictions['Forecast'] = se[0]\n",
    "\n",
    "# Create dates df index\n",
    "df1 = df['date'].tail(len(y_test))\n",
    "\n",
    "# Concatenate both dataframes\n",
    "pred = pd.concat([predictions, df1], axis=1)\n",
    "\n",
    "# Set index\n",
    "pred.set_index('date', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(pred)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel(\"Price\")\n",
    "plt.legend([\"Real Price\", \"Prediction\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model to file\n",
    "#joblib.dump(model, \"./xgboost.dat\")\n",
    "# The sklearn API models are picklable\n",
    "print(\"Pickling sklearn API models\")\n",
    "# must open in binary format to pickle\n",
    "pickle.dump(xg_reg, open(\"./xgboost.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
